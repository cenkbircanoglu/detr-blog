{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16574f8f-0396-4243-b102-950816ed413d",
   "metadata": {},
   "source": [
    "## Summary of the paper\n",
    "\"End-to-End Object Detection with Transformers\" describes a computer vision methodology that utilizes transformer-based architectures for direct object detection, eliminating the requirement for conventional techniques such as region proposal networks.\n",
    "\n",
    "The approach involves adapting transformer architecture, originally designed for natural language processing, to address object detection tasks. Transformers, valued for their proficiency in capturing extensive dependencies in sequential data, demonstrate suitability for tasks extending beyond language processing.\n",
    "\n",
    "The architecture adopts a typical encoder-decoder structure prevalent in sequence-to-sequence tasks. The encoder handles input data (image features), extracting relevant information, while the decoder generates predictions like bounding boxes and class labels using the encoded details.\n",
    "\n",
    "As transformers operate on 2D dimensions, a flattening operation is necessary for processing image data within the model.\n",
    "\n",
    "Positional embeddings are incorporated into the model to convey details about the spatial arrangement of elements in the input data.\n",
    "Positional embeddings are generated to augment the transformer's capability to comprehend spatial relationships within the image. There are several possible positional embeddings generation such as sine based, learnable etc.\n",
    "\n",
    "The model uses object queries to focus on specific regions of the input image implicitly. Object queries act as learnable parameters that guide the attention mechanism to relevant parts of the image, contributing to accurate object localization.\n",
    "\n",
    "The model is trained end-to-end, optimizing parameters to minimize the difference between predicted and ground truth bounding boxes and class labels.\n",
    "\n",
    "Unlike traditional object detection methods, the model eliminates the need for non-maximum suppression (NMS) during post-processing.\n",
    "The model can directly output multiple bounding boxes for a single object, addressing the need for selecting the most confident prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149a89f-864c-4a3c-b43a-a3aeef4155a2",
   "metadata": {},
   "source": [
    "In this notebook, I delve into the original implementation of the DEtection TRansformer (DETR) model from the \"End-to-End Object Detection with Transformers\" paper, accessible on [GitHub](https://github.com/facebookresearch/detr). To enhance the model's comprehensibility, I've organized it into distinct components:\n",
    "\n",
    "- Batch Creation\n",
    "- Feature Map Extraction from CNN\n",
    "- Addition of Positional Embeddings to Feature Maps\n",
    "- Acquisition of High-Level Activation Maps\n",
    "- Transformer Encoder (Single Layer/Multi-Layer)\n",
    "- Transformer Decoder (Single Layer/Multi-Layer)\n",
    "- MLP Layer for BBox Prediction\n",
    "- Linear Layer for Class Prediction\n",
    "\n",
    "Additionally, I have excluded the aux_loss from the original implementation to simplify the code. Towards the end, I assembled the DETR model using the modified modules. It's important to note that this code is not written entirely from scratch; rather, modifications have been made for improved clarity.\n",
    "\n",
    "As a note, object detection set prediction loss which makes this architrecture works is another important part. However, to focus on one part, I didn't add the implementation of the loss function to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419e5780-29aa-462d-af2a-0e86f15b0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033d5e3a-0cf4-407c-895d-ece7d664f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install ipyplot pillow torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f675b8-4484-4ffd-a7a0-91809af95c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e89e413-5872-41f8-920e-530ab64a9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the microwave image: (425, 639) and car image: (640, 622)\n"
     ]
    }
   ],
   "source": [
    "# Read two images from the disk\n",
    "from PIL import Image\n",
    "microwave = Image.open('./images/microwave.jpeg')\n",
    "car = Image.open('./images/car.jpeg')\n",
    "print(f'Dimensions of the microwave image: {microwave.size} and car image: {car.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd96c550-51bb-45e7-ab33-205a89a93d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pillow images to torch tensor\n",
    "import torchvision.transforms as T\n",
    "microwave_tensor = T.PILToTensor()(microwave)\n",
    "car_tensor = T.PILToTensor()(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0445ad4e-4043-4271-8fe7-1a209fb6bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensor list\n",
    "image_list = [microwave_tensor, car_tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67b888-6132-4591-a4f2-52bec2b94344",
   "metadata": {},
   "source": [
    "### Create Batches\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/input_batch.png' width=70% height=50%/>,\n",
    "</div>\n",
    "As mentioned in the paper with the preceding sentence, the initial step involves determining the maximum height and width among the images in the batch. Subsequently, apply zero-padding to the original images to ensure uniform dimensions. Additionally, generate a mask tensor indicating whether each image has undergone zero-padding or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8904bc-11c9-401e-a183-f668bf467600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_and_create_masks(batch):\n",
    "    # Find the maximum Height and width\n",
    "    max_height, max_width = max(image.shape[1] for image in batch), max(image.shape[2] for image in batch)\n",
    "    print(f'Max Width and Height value in the batch: {max_width}, {max_height}')\n",
    "\n",
    "    # Initialise a zero tensor with the dimension of desired batch\n",
    "    padded_batch = torch.zeros((len(batch), 3, max_height, max_width), dtype=batch[0].dtype)\n",
    "    # Initialise a one-tensor with the dimension of desired batch\n",
    "    mask_batch = torch.ones((len(batch), max_height, max_width), dtype=torch.bool)\n",
    "\n",
    "    # Copy the images to padded_batch and set the mask 0 for the pixels which is not in the original image\n",
    "    for image, pad_image, mask in zip(batch, padded_batch, mask_batch):\n",
    "        pad_image[:, :image.shape[1], :image.shape[2]].copy_(image)\n",
    "        mask[:image.shape[1], :image.shape[2]] = False\n",
    "\n",
    "    return padded_batch, mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8df3eb-006d-41df-add6-c46b0fe56151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Width and Height value in the batch: 640, 639\n",
      "Shape of the Padded Batch [2, 3, 639, 640], Shape of the masks: [2, 639, 640]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the padded_batch and mask_batch from image_list\n",
    "padded_batch, mask_batch = resize_images_and_create_masks(image_list)\n",
    "print(f'Shape of the Padded Batch {list(padded_batch.shape)}, Shape of the masks: {list(mask_batch.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62102b-cf3a-45e7-8011-39a5473deece",
   "metadata": {},
   "source": [
    "As it is shown here, the images are 0 padded according to the new image dimensions\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/batch.png' width=70% height=50%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691288b-c2bb-4e78-9445-574028d74554",
   "metadata": {},
   "source": [
    "## Overall Architecture\n",
    "\n",
    "The overall architecture of DETR model contains:\n",
    "\n",
    "- Image Encoder (Backbone): The input image passes through a pre-trained CNN model (ResNet50), without dense and pooling layers, to extract feature maps. These feature maps capture hierarchical information about the input image, from low-level to high-level features.\n",
    "\n",
    "- Positional Encoding: The feature maps are augmented with positional embeddings to provide spatial information to the model.\n",
    "The positional embeddings are added to the feature maps, allowing the model to take into account the absolute position of each element.\n",
    "\n",
    "- Object Queries: A set of learnable object queries is introduced. Each query represents a potential object in the image. \n",
    "\n",
    "- Transformer Encoder: The augmented feature maps, masks, and positional encodings,  are passed through a transformer encoder.\n",
    "The transformer encoder processes the input in a self-attention mechanism, allowing the model to capture global dependencies and relationships.\n",
    "\n",
    "- Decoder: The output of the transformer encoder is fed into the decoder with masks, positional embeddings, object queries, target embeddings and query embeddings. The decoder consists of two transformer decoder layers. Each decoder layer refines the object queries and generates predictions for the bounding boxes and class labels.\n",
    "\n",
    "- Class Predictions: A set of probability scores indicating the presence of each class for each object query.\n",
    "- Box Predictions: Predictions for the bounding boxes (coordinates) of the detected objects.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/architecture.png' width=70% height=50%/>,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aaa793-4723-48d4-b1ae-06b8162ff367",
   "metadata": {},
   "source": [
    "### Extract Feature Maps\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/backbone.png' width=70% height=50%/>,\n",
    "</div>\n",
    "In the paper, the authors mentioned exactly \"the backbone is with ImageNet-pretrained ResNet model [15] from torchvision with frozen batchnorm layers.\"\n",
    "\n",
    "The original implementation contains extracting the intermediate block outputs also. And "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49226016-ce8e-4f73-b746-fc5e706b0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Backbone model from Pretrained ResNet50\n",
    "import torchvision\n",
    "from torchvision.ops import FrozenBatchNorm2d\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # get the pre-trained ResNet50 without classifier and pooling layer\n",
    "        self.backbone = nn.Sequential(*list(torchvision.models.resnet50(pretrained=True, norm_layer=FrozenBatchNorm2d).children())[:-2])\n",
    "        self.num_channels = 2048\n",
    "\n",
    "    def forward(self, padded_batch, mask_batch):\n",
    "        # extract the features from batch\n",
    "        features = self.backbone(padded_batch)\n",
    "        # Rescale masks to feature size\n",
    "        feature_masks = F.interpolate(mask_batch.unsqueeze(0).float(), size=features.shape[-2:]).to(torch.bool)[0]\n",
    "        return features, feature_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7d0957-675e-48c7-bd21-9af6d61ae85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature maps shape: [2, 2048, 20, 20] and feature masks shape: [2, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "backbone = Backbone()\n",
    "padded_batch = padded_batch.float()\n",
    "features, feature_masks = backbone(padded_batch, mask_batch)\n",
    "print(f'Feature maps shape: {list(features.shape)} and feature masks shape: {list(feature_masks.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5ec9-d14e-4540-9b5d-2b6c7cbb7b86",
   "metadata": {},
   "source": [
    "### Add Positional Embeddings \n",
    "Positional embeddings are added to the feature maps to provide spatial information to the model. These embeddings help the transformer understand the positional relationships between different regions in the image.\n",
    "\n",
    "The fixed positional embeddings are calculated using sine and cosine functions of different frequencies. These embeddings are then added to the input features. The choice of sine and cosine functions allows the model to capture periodic patterns in the positional information.\n",
    "\n",
    "For a given position i and dimension d, the positional embedding is calculated as follows:\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/positional_embedding_sin.png' width=50% height=50%/>,\n",
    "</div>\n",
    "where D is the embedding dimension.\n",
    "\n",
    "These fixed positional embeddings are then added to the input features before feeding them into the transformer model. This helps the model learn spatial relationships between different positions, even in scenarios where the input lacks explicit spatial information, such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77583af8-2178-4944-aa2c-d82d77e67221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise PositionEmbeddingSine class \n",
    "class PositionEmbeddingSine(nn.Module):\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.scale = 2 * math.pi\n",
    "\n",
    "    def forward(self, feature_masks):\n",
    "        not_mask = ~feature_masks\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        # Normalization of cumulative sums to ensure values are within a specified scale.\n",
    "        eps = 1e-6\n",
    "        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        # dim_t: A tensor representing the positional dimensions.\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32)\n",
    "        # Exponential scaling of dimensions based on the temperature hyperparameter.\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "        \n",
    "        # Computation of positional embeddings for both x and y dimensions using sine and cosine functions.\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        \n",
    "        # Stack and flatten the sine and cosine components.\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        # Concatenate x and y embeddings and permute dimensions to match the expected format.\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0195445a-71e0-4622-9511-e534b34b67b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the positional embeddings: [2, 256, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "# initialise position_embed module and generate embeddings from masks\n",
    "embedding_dim = 256\n",
    "N_steps = embedding_dim // 2\n",
    "\n",
    "position_embed = PositionEmbeddingSine(N_steps)\n",
    "positional_embeddings = position_embed(feature_masks).to(features.dtype)\n",
    "print(f'Shape of the positional embeddings: {list(positional_embeddings.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9783bd-0bb9-42cc-88c7-f364b65d07fd",
   "metadata": {},
   "source": [
    "### High-level activation map\n",
    "\n",
    "This operation is used to project the high-dimensional features into a lower-dimensional space, making it suitable for subsequent processing by the transformer layers. It allows the model to capture more abstract and compact representations of the input information.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='https://raw.githubusercontent.com/cenkbircanoglu/detr-blog/main/images/high_level_activations.png' width=50% height=50%/>,\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f93e3b-84b9-4ba1-8e29-1ed9ac9a6e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-level activation maps: [2, 256, 20, 20] created from features: [2, 2048, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "# 1x1 convolution \n",
    "conv = nn.Conv2d(backbone.num_channels, embedding_dim, kernel_size=1)\n",
    "activation_maps = conv(features)\n",
    "print(f'High-level activation maps: {list(activation_maps.shape)} created from features: {list(features.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaca770-470c-474f-8fba-38958bfb8bca",
   "metadata": {},
   "source": [
    "### Flatten Inputs\n",
    "Since the transformer architecture doesn't accept images or patches as they are, the activation maps, positional embeddings, and feature masks are flattened as shown below. The spatial dimensions are collapsed into a single dimension, effectively transforming the 2D feature map into a sequence of tokens suitable for input to the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfba55df-3855-4ad0-b4f4-5d21628cb93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened and original activation maps shapes:[400, 2, 256], [2, 256, 20, 20] \n",
      "Flattened and original positional embeddings shapes:[400, 2, 256], [2, 256, 20, 20] \n",
      "Flattened and original feature masks shapes:[2, 400], [2, 20, 20] \n"
     ]
    }
   ],
   "source": [
    "bs, c, h, w = activation_maps.shape\n",
    "flattened_activation_maps = activation_maps.flatten(2).permute(2, 0, 1)\n",
    "flattened_positional_embeddings = positional_embeddings.flatten(2).permute(2, 0, 1)\n",
    "flattened_feature_masks = feature_masks.flatten(1)\n",
    "\n",
    "print(f'Flattened and original activation maps shapes:{list(flattened_activation_maps.shape)}, {list(activation_maps.shape)} ')\n",
    "print(f'Flattened and original positional embeddings shapes:{list(flattened_positional_embeddings.shape)}, {list(positional_embeddings.shape)} ')\n",
    "print(f'Flattened and original feature masks shapes:{list(flattened_feature_masks.shape)}, {list(feature_masks.shape)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ca3f8-5e47-46cd-91cd-b95ec2e3ef3e",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "It aims to capture diverse relationships and dependencies within the input data by allowing the model to attend to different parts of the input sequence independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834582fb-3086-4aec-a5c8-c4ffce2b5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MultiHeadAttention class used in the original implementation\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.in_proj_weight = nn.Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "        self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))\n",
    "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_padding_mask=None, attn_mask=None):\n",
    "\n",
    "        # Allows the model to jointly attend to information from different representation subspaces as described \n",
    "        # in the paper: `Attention Is All You Need`.\n",
    "        # Self-attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
    "        attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "            query, key, value, self.embed_dim, self.num_heads,\n",
    "            self.in_proj_weight, self.in_proj_bias,\n",
    "            None, None, False,\n",
    "            self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            attn_mask=attn_mask)\n",
    "        return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cf18bd3-c2f3-4f9c-9292-bda8f0931b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: [400, 2, 256] and Attention output weight shape: [2, 400, 400]\n"
     ]
    }
   ],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "self_attn = MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "\n",
    "q = k = flattened_activation_maps + flattened_positional_embeddings\n",
    "attn_output, attn_output_weights = self_attn(q, k, value=flattened_activation_maps, key_padding_mask=flattened_feature_masks)\n",
    "print(f'Attention output shape: {list(attn_output.shape)} and Attention output weight shape: {list(attn_output_weights.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95154f27-43a4-4547-990d-59906e0ca9d4",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "The feature maps with positional embeddings are fed into a transformer encoder. The encoder processes the spatial information in parallel for different regions of the image. DETR uses a multi-head self-attention mechanism in the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d6cb0d-98f0-4b7c-9b47-a5b8eccf2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Single Layered Encoder module\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, features, feature_mask, positional_embeddings):\n",
    "        q = k = features + positional_embeddings\n",
    "        src2 = self.self_attn(q, k, value=features, key_padding_mask=feature_mask)[0]\n",
    "        features = features + self.dropout1(src2)\n",
    "        features = self.norm1(features)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(features))))\n",
    "        features = features + self.dropout2(src2)\n",
    "        features = self.norm2(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0638c577-18a6-4575-ab24-0bd782f0720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: [400, 2, 256]\n"
     ]
    }
   ],
   "source": [
    "dropout=0.1\n",
    "hidden_dim=2048\n",
    "encoder_layer = EncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
    "\n",
    "single_layer_encoder_output = encoder_layer(flattened_activation_maps, flattened_feature_masks, flattened_positional_embeddings)\n",
    "print(f'Encoder output shape: {list(single_layer_encoder_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a127469-28a4-4324-b8cf-c85d2941780f",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "The transformer decoder takes the output of the encoder and generates high-level feature representations. It utilizes multi-head self-attention and cross-attention mechanisms to capture both spatial and contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9a4080-8757-475c-8ab9-06422ddf700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Single Layered Decoder module\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        self.multihead_attn = MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, target, encoder_outputs, feature_masks, positional_embeddings, query_embeddings):\n",
    "        q = k = target + query_embeddings\n",
    "        tgt2 = self.self_attn(q, k, value=target)[0]\n",
    "        target = target + self.dropout1(tgt2)\n",
    "        target = self.norm1(target)\n",
    "        tgt2 = self.multihead_attn(query=target + query_embeddings,\n",
    "                                   key=encoder_outputs + positional_embeddings,\n",
    "                                   value=encoder_outputs, key_padding_mask=feature_masks)[0]\n",
    "        target = target + self.dropout2(tgt2)\n",
    "        target = self.norm2(target)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(target))))\n",
    "        target = target + self.dropout3(tgt2)\n",
    "        target = self.norm3(target)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbfde7-a7fd-4ac3-9d73-4fd5ce7b0b3a",
   "metadata": {},
   "source": [
    "#### Object Queries\n",
    "Object queries are associated with specific object classes that the model is trained to recognize. The attention mechanism guided by these queries helps the model determine the presence of objects and predict their corresponding class labels.\n",
    "The attended features from different object queries are used collectively to generate predictions for both object localization (bounding boxes) and object classification (class labels). The model leverages the information gathered by object queries to make accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee115268-521a-46d2-88d4-31b8c94c706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: [100, 2, 256]\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
    "\n",
    "num_queries = 100\n",
    "query_embeddings = nn.Embedding(num_queries, embedding_dim).weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "\n",
    "target = torch.zeros_like(query_embeddings)\n",
    "single_layer_decoder_output = decoder_layer(target, single_layer_encoder_output, flattened_feature_masks, flattened_positional_embeddings, query_embeddings)\n",
    "print(f'Decoder output shape: {list(single_layer_decoder_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a490ddd-4deb-4913-90e7-b4f3d431c0e0",
   "metadata": {},
   "source": [
    "### Multi-Layer Encoder Model\n",
    "The feature maps with positional embeddings are fed into a transformer encoder. The encoder processes the spatial information in parallel for different regions of the image. DETR uses a multi-head self-attention mechanism in the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0c8189c-1ded-4248-8971-187f935ad4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer encoder model which only creates N encoder layer and feeds each other in the execution step.\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        encoder_layer = EncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, features, feature_mask, positional_embeddings):\n",
    "        output = features\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, feature_mask, positional_embeddings)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0e4fc9-9890-4c3f-b424-724208eb7a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: [400, 2, 256]\n"
     ]
    }
   ],
   "source": [
    "num_encoder_layers=6\n",
    "encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout, num_encoder_layers)\n",
    "\n",
    "encoder_output = encoder(flattened_activation_maps, flattened_feature_masks, flattened_positional_embeddings)\n",
    "assert encoder_output.shape == single_layer_encoder_output.shape\n",
    "print(f'Encoder output shape: {list(encoder_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7783ff-3668-42aa-bb1d-8380794279f2",
   "metadata": {},
   "source": [
    "### Multi head Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bcbee59-2962-407d-8e8f-415400c7f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer decoder model which only creates N decoder layer and feeds each other in the execution step.\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        decoder_layer = DecoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, target, encoder_outputs, feature_masks, positional_embeddings, query_embeddings):\n",
    "        output = target\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, encoder_outputs, feature_masks,positional_embeddings, query_embeddings)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e73a832-c674-409d-9244-24f0cfd08892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: [1, 100, 2, 256]\n"
     ]
    }
   ],
   "source": [
    "num_decoder_layers=6\n",
    "decoder = TransformerDecoder(embedding_dim, num_heads, hidden_dim, dropout, num_encoder_layers)\n",
    "decoder_output = decoder(target, encoder_output, flattened_feature_masks, flattened_positional_embeddings, query_embeddings)\n",
    "assert decoder_output[0].shape == single_layer_decoder_output.shape \n",
    "print(f'Decoder output shape: {list(decoder_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdbec0c-05e1-49c7-89b4-0e1da286a508",
   "metadata": {},
   "source": [
    "### Transformer Model\n",
    "The transformer model combines both encoder and decoder components in its architecture and performs the flattening operations within its forward function. The forward function is a crucial part of the model where input data is processed, and predictions are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7bd1174-5d02-4a5e-b728-180d40e0b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce Transformer model\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=512, num_heads=8, hidden_dim=2048, dropout=0.1, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout, num_encoder_layers)\n",
    "        self.decoder = TransformerDecoder(embedding_dim, num_heads, hidden_dim, dropout, num_decoder_layers)\n",
    "\n",
    "    def forward(self, features, feature_masks, query_embeddings, positional_embeddings):\n",
    "        bs, c, h, w = features.shape\n",
    "        features = features.flatten(2).permute(2, 0, 1)\n",
    "        positional_embeddings = positional_embeddings.flatten(2).permute(2, 0, 1)\n",
    "        query_embeddings = query_embeddings.unsqueeze(1).repeat(1, bs, 1)\n",
    "        feature_masks = feature_masks.flatten(1)\n",
    "\n",
    "        target = torch.zeros_like(query_embeddings)\n",
    "        encoder_output = self.encoder(features, feature_masks, positional_embeddings)\n",
    "        decoder_output = self.decoder(target, encoder_output, feature_masks, positional_embeddings, query_embeddings)\n",
    "        return decoder_output.transpose(1, 2), encoder_output.permute(1, 2, 0).view(bs, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11ac016-6aad-446d-9826-51f2a7b2b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder output: [1, 2, 100, 256] and encoder output: [2, 256, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "# initialise transformer model\n",
    "num_decoder_layers = 6\n",
    "num_encoder_layers = 6\n",
    "dropout = 0.1\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "embedding_dim = 256\n",
    "\n",
    "transformer = Transformer(embedding_dim=embedding_dim, dropout=dropout, num_heads=num_heads, hidden_dim=hidden_dim, \n",
    "                          num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "\n",
    "# initialise Query Embedding\n",
    "num_queries = 100\n",
    "query_embeddings = nn.Embedding(num_queries, embedding_dim).weight\n",
    "\n",
    "# execute transformer model\n",
    "decoder_output, encoder_output = transformer(activation_maps, feature_masks, query_embeddings, positional_embeddings)\n",
    "print(f'Shape of decoder output: {list(decoder_output.shape)} and encoder output: {list(encoder_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a477c-4e45-464d-8db1-af10ab99da2f",
   "metadata": {},
   "source": [
    "### Classification head\n",
    "A linear layer is used for predicting the class probabilities for each bounding box. It takes the decoder's high-level features as input and outputs class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "328c3776-6c5d-4cba-8082-fb2bda134558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier outputs: [1, 2, 100, 21]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 20\n",
    "class_embed = nn.Linear(embedding_dim, num_classes + 1)\n",
    "classifier_output = class_embed(decoder_output)\n",
    "print(f'Classifier outputs: {list(classifier_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436d2e9-7f23-4a54-9193-8ea924f81c1f",
   "metadata": {},
   "source": [
    "### MLP Model\n",
    "The MLP (Multi-Layer Perceptron) head is responsible for predicting bounding box coordinates. It takes the high-level features from the decoder and produces bounding box coordinates for each object in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4beb2938-6f30-426f-9421-79a9b1a6f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d5ef7b-3f40-42dc-9de1-933b3b439848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP outputs: [1, 2, 100, 4]\n"
     ]
    }
   ],
   "source": [
    "bbox_embed = MLP(embedding_dim, embedding_dim, 4, 3)\n",
    "mlp_output = bbox_embed(decoder_output).sigmoid()\n",
    "print(f'MLP outputs: {list(mlp_output.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff85388-c03d-4767-983f-5957d643531a",
   "metadata": {},
   "source": [
    "### DETR Model\n",
    "\n",
    "At the end, we can create the DETR model by combining the models defined above according to the what authors described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "208eb62c-ef39-480d-88d1-076b732ebd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048, dropout=0.1, num_heads=8, hidden_dim=2048, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, num_classes=20, num_queries=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialiase backbone\n",
    "        self.backbone = Backbone()\n",
    "        \n",
    "        # Initialise PositionEmbeddings\n",
    "        N_steps = embedding_dim // 2\n",
    "        self.position_embedding = PositionEmbeddingSine(N_steps)\n",
    "        \n",
    "        # Initialise Transformer Model\n",
    "        self.transformer = Transformer(embedding_dim=embedding_dim, dropout=dropout, num_heads=num_heads, hidden_dim=hidden_dim, \n",
    "                                       num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "        \n",
    "        # initialise classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes + 1)\n",
    "\n",
    "        # initialise bounding box model\n",
    "        self.bbox_mlp = MLP(embedding_dim, embedding_dim, 4, 3)\n",
    "\n",
    "        # initialise query embeddings\n",
    "        self.query_embeddings = nn.Embedding(num_queries, embedding_dim)\n",
    "        self.input_proj = nn.Conv2d(backbone.num_channels, embedding_dim, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Resize the batch and create masks\n",
    "        padded_batch, mask_batch = resize_images_and_create_masks(batch)\n",
    "        padded_batch = padded_batch.float()\n",
    "        \n",
    "        # Extract features from backbone\n",
    "        features, feature_masks = self.backbone(padded_batch, mask_batch)\n",
    "\n",
    "        # initialise positional embeddings\n",
    "        positional_embeddings = self.position_embedding(feature_masks).to(features.dtype)\n",
    "\n",
    "        # execute transformer model\n",
    "        decoder_output, encoder_output = self.transformer(self.input_proj(features), feature_masks, self.query_embeddings.weight, positional_embeddings)\n",
    "\n",
    "        # classify decoder outputs\n",
    "        classifier_output = self.classifier(decoder_output)\n",
    "        \n",
    "        # execute bounding box mlp model\n",
    "        mlp_output = self.bbox_mlp(decoder_output).sigmoid()\n",
    "        \n",
    "        out = {'pred_logits': classifier_output[-1], 'pred_boxes': mlp_output[-1]}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3944aa67-1106-4235-8e32-1f8ffe99efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Width and Height value in the batch: 640, 639\n",
      "Predicted logits and predicted boxes have shapes: [2, 100, 21], [2, 100, 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialise the model\n",
    "model = DETR(embedding_dim=2048, dropout=0.1, num_heads=8, hidden_dim=2048, num_encoder_layers=6, num_decoder_layers=6, \n",
    "        num_classes=20, num_queries=100)\n",
    "\n",
    "outputs = model(image_list)\n",
    "print(f\"Predicted logits and predicted boxes have shapes: {list(outputs['pred_logits'].shape)}, {list(outputs['pred_boxes'].shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1de3d-0c8e-4763-b1ee-4696b5acdaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225105c7-cc76-4387-a481-f0da5a3ad58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
